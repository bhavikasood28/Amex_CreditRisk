{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c0bdf0-27e5-4304-9f2d-6ce98c4a9e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Professional Feature Engineering Pipeline...\n",
      "Reading train_data.csv in chunks...\n",
      "  Processing Chunk 1...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 2...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 3...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 4...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 5...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 6...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 7...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 8...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 9...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 10...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 11...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 12...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 13...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 14...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 15...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 16...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 17...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 18...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 19...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 20...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 21...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 22...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 23...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 24...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 25...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 26...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 27...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Chunk 28...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
      "/var/folders/80/qns4qhrj56lg74phvxs3sj0r0000gn/T/ipykernel_33310/2516221810.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  chunk_agg.reset_index(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenating chunks...\n",
      "Final Groupby to merge split customers...\n",
      "Merging with Labels...\n",
      "Saving to customer_features_enhanced.csv...\n",
      "ðŸŽ‰ SUCCESS! Pipeline finished in 3.0 minutes.\n",
      "Final Data Shape: (458913, 556)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# 1. Setup\n",
    "TRAIN_FILE = \"train_data.csv\"\n",
    "LABEL_FILE = \"train_labels.csv\"\n",
    "OUTPUT_FILE = \"customer_features_enhanced.csv\"\n",
    "\n",
    "def process_advanced_features():\n",
    "    start_time = time.time()\n",
    "    print(\"ðŸš€ Starting Professional Feature Engineering Pipeline...\")\n",
    "    \n",
    "    # Define Column Groups\n",
    "    cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', \n",
    "                'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "    \n",
    "    chunksize = 200000 \n",
    "    chunk_list = []\n",
    "\n",
    "    print(f\"Reading {TRAIN_FILE} in chunks...\")\n",
    "    \n",
    "    with pd.read_csv(TRAIN_FILE, chunksize=chunksize) as reader:\n",
    "        for i, chunk in enumerate(reader):\n",
    "            print(f\"  Processing Chunk {i+1}...\", end=\"\\r\")\n",
    "\n",
    "            # âœ” FIX: Sort so 'last' means truly LAST chronologically\n",
    "            chunk = chunk.sort_values([\"customer_ID\", \"S_2\"])\n",
    "\n",
    "            # Identify numeric columns\n",
    "            num_cols = [c for c in chunk.columns if c not in cat_cols + ['customer_ID', 'S_2']]\n",
    "            \n",
    "            # Aggregation logic\n",
    "            chunk_agg = chunk.groupby('customer_ID').agg({\n",
    "                **{c: ['mean', 'max', 'last'] for c in num_cols},\n",
    "                **{c: ['last', 'nunique'] for c in cat_cols}\n",
    "            })\n",
    "            \n",
    "            # Flatten columns\n",
    "            chunk_agg.columns = ['_'.join(x) for x in chunk_agg.columns]\n",
    "            \n",
    "            # Statement count\n",
    "            chunk_agg['statement_count'] = chunk.groupby('customer_ID')['S_2'].count()\n",
    "            \n",
    "            chunk_agg.reset_index(inplace=True)\n",
    "            chunk_list.append(chunk_agg)\n",
    "\n",
    "            # Safety test option:\n",
    "            # if i == 1: break\n",
    "    \n",
    "    print(\"\\nConcatenating chunks...\")\n",
    "    full_df = pd.concat(chunk_list, axis=0)\n",
    "    \n",
    "    print(\"Final Groupby to merge split customers...\")\n",
    "    final_features = full_df.groupby('customer_ID').last().reset_index()\n",
    "\n",
    "    print(\"Merging with Labels...\")\n",
    "    labels = pd.read_csv(LABEL_FILE)\n",
    "    final_dataset = final_features.merge(labels, on='customer_ID', how='inner')\n",
    "    \n",
    "    print(f\"Saving to {OUTPUT_FILE}...\")\n",
    "    final_dataset.to_csv(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"ðŸŽ‰ SUCCESS! Pipeline finished in {elapsed:.1f} minutes.\")\n",
    "    print(f\"Final Data Shape: {final_dataset.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_advanced_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf565114-a1ee-4411-98f1-ee383e354492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>P_2_mean</th>\n",
       "      <th>P_2_max</th>\n",
       "      <th>P_2_last</th>\n",
       "      <th>D_39_mean</th>\n",
       "      <th>D_39_max</th>\n",
       "      <th>D_39_last</th>\n",
       "      <th>B_1_mean</th>\n",
       "      <th>B_1_max</th>\n",
       "      <th>B_1_last</th>\n",
       "      <th>...</th>\n",
       "      <th>D_63_last</th>\n",
       "      <th>D_63_nunique</th>\n",
       "      <th>D_64_last</th>\n",
       "      <th>D_64_nunique</th>\n",
       "      <th>D_66_last</th>\n",
       "      <th>D_66_nunique</th>\n",
       "      <th>D_68_last</th>\n",
       "      <th>D_68_nunique</th>\n",
       "      <th>statement_count</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0.933824</td>\n",
       "      <td>0.960384</td>\n",
       "      <td>0.934745</td>\n",
       "      <td>0.010704</td>\n",
       "      <td>0.091505</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>0.021655</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>...</td>\n",
       "      <td>CR</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>0.899820</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>0.880519</td>\n",
       "      <td>0.215205</td>\n",
       "      <td>0.567403</td>\n",
       "      <td>0.178126</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.109644</td>\n",
       "      <td>0.034684</td>\n",
       "      <td>...</td>\n",
       "      <td>CO</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>0.878454</td>\n",
       "      <td>0.904482</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>0.004181</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>0.009704</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>0.009997</td>\n",
       "      <td>0.004284</td>\n",
       "      <td>...</td>\n",
       "      <td>CO</td>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>0.598969</td>\n",
       "      <td>0.623392</td>\n",
       "      <td>0.621776</td>\n",
       "      <td>0.048862</td>\n",
       "      <td>0.268476</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.059876</td>\n",
       "      <td>0.279991</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>...</td>\n",
       "      <td>CO</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>0.891679</td>\n",
       "      <td>0.940382</td>\n",
       "      <td>0.871900</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>0.008680</td>\n",
       "      <td>0.005573</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.009806</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>...</td>\n",
       "      <td>CO</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  P_2_mean   P_2_max  \\\n",
       "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  0.933824  0.960384   \n",
       "1  00000fd6641609c6ece5454664794f0340ad84dddce9a2...  0.899820  0.929122   \n",
       "2  00001b22f846c82c51f6e3958ccd81970162bae8b007e8...  0.878454  0.904482   \n",
       "3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723...  0.598969  0.623392   \n",
       "4  00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...  0.891679  0.940382   \n",
       "\n",
       "   P_2_last  D_39_mean  D_39_max  D_39_last  B_1_mean   B_1_max  B_1_last  \\\n",
       "0  0.934745   0.010704  0.091505   0.009119  0.012007  0.021655  0.009382   \n",
       "1  0.880519   0.215205  0.567403   0.178126  0.025654  0.109644  0.034684   \n",
       "2  0.880875   0.004181  0.009704   0.009704  0.004386  0.009997  0.004284   \n",
       "3  0.621776   0.048862  0.268476   0.001083  0.059876  0.279991  0.012564   \n",
       "4  0.871900   0.004644  0.008680   0.005573  0.005941  0.009806  0.007679   \n",
       "\n",
       "   ...  D_63_last  D_63_nunique  D_64_last  D_64_nunique  D_66_last  \\\n",
       "0  ...         CR             1          O             1        NaN   \n",
       "1  ...         CO             1          O             1        NaN   \n",
       "2  ...         CO             1          R             1        NaN   \n",
       "3  ...         CO             1          O             1        NaN   \n",
       "4  ...         CO             1          O             1        1.0   \n",
       "\n",
       "   D_66_nunique  D_68_last  D_68_nunique  statement_count  target  \n",
       "0             0        6.0             1               13       0  \n",
       "1             0        6.0             1               13       0  \n",
       "2             0        6.0             1               13       0  \n",
       "3             0        3.0             3               13       0  \n",
       "4             1        6.0             1               13       0  \n",
       "\n",
       "[5 rows x 556 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"customer_features_enhanced.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12268c6-85d6-40f8-88ba-4a9bec79b0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
